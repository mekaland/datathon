{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UaT7Zz8xxRZA"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, pandas as pd, numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DCkvULOrxfTE",
        "outputId": "3a492923-2c50-4215-8511-a49c1c966522"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri yükleme (sizin kodunuzdan)\n",
        "BASE = \"/content/sample_data/datathon2025\"\n",
        "OUT = os.path.join(BASE, \"outputs\")\n",
        "ARTI = os.path.join(OUT, \"artifacts\")\n",
        "\n",
        "df = pd.read_csv(os.path.join(BASE, \"train.csv\"))\n",
        "test = pd.read_csv(os.path.join(BASE, \"test.csv\"))\n",
        "sub = pd.read_csv(os.path.join(BASE, \"sample_submission.csv\"))"
      ],
      "metadata": {
        "id": "pvC1a23DxhW8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, pandas as pd\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "BASE = \"/content/sample_data/datathon2025\"\n",
        "OUT  = os.path.join(BASE, \"outputs\"); os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "df   = pd.read_csv(os.path.join(BASE, \"train.csv\"))\n",
        "test = pd.read_csv(os.path.join(BASE, \"test.csv\"))\n",
        "sub  = pd.read_csv(os.path.join(BASE, \"sample_submission.csv\"))\n",
        "\n",
        "# Zaman alanını düzgünleştir\n",
        "for d in (df, test):\n",
        "    d[\"event_time\"] = pd.to_datetime(d[\"event_time\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "print(df.shape, test.shape, sub.shape)\n",
        "print(df.head(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB_K6kRBxnBY",
        "outputId": "91f05d8f-6cac-4ca3-c21e-6937c7156726"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(141219, 7) (62951, 6) (30789, 2)\n",
            "                 event_time event_type   product_id category_id      user_id  \\\n",
            "0 2025-06-19 10:23:07+00:00   ADD_CART  PROD_011223   CAT_00054  USER_097562   \n",
            "1 2025-06-07 21:34:45+00:00   ADD_CART  PROD_005519   CAT_00144  USER_006535   \n",
            "\n",
            "     user_session  session_value  \n",
            "0  SESSION_158779          90.29  \n",
            "1  SESSION_029987          16.39  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sv_per_sess = df.groupby(\"user_session\")[\"session_value\"].nunique(dropna=False)\n",
        "print(\"Tekil session_value oranı:\", (sv_per_sess==1).mean())\n",
        "print(\"Farklı value taşıyan oturum sayısı:\", int((sv_per_sess>1).sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_opFFO2xnJc",
        "outputId": "249cc852-ef2a-4d51-cd8a-df01db0e14ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekil session_value oranı: 1.0\n",
            "Farklı value taşıyan oturum sayısı: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, d in [(\"TRAIN\", df), (\"TEST\", test)]:\n",
        "    c = d.groupby(\"user_session\").size()\n",
        "    print(f\"{name}  oturum sayısı: {c.shape[0]:,} | toplam event: {d.shape[0]:,}\")\n",
        "    print(c.describe(percentiles=[.1,.25,.5,.75,.9]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CleBaOIxnML",
        "outputId": "a077e9ed-59cb-4d40-cbfb-89da5ec0f933"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN  oturum sayısı: 70,736 | toplam event: 141,219\n",
            "count    70736.000000\n",
            "mean         1.996423\n",
            "std          2.579703\n",
            "min          1.000000\n",
            "10%          1.000000\n",
            "25%          1.000000\n",
            "50%          1.000000\n",
            "75%          2.000000\n",
            "90%          4.000000\n",
            "max        116.000000\n",
            "dtype: float64\n",
            "TEST  oturum sayısı: 30,789 | toplam event: 62,951\n",
            "count    30789.000000\n",
            "mean         2.044594\n",
            "std          2.508177\n",
            "min          1.000000\n",
            "10%          1.000000\n",
            "25%          1.000000\n",
            "50%          1.000000\n",
            "75%          2.000000\n",
            "90%          4.000000\n",
            "max         87.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Satır düzeyi sayımlar:\")\n",
        "print(df[\"event_type\"].value_counts())\n",
        "evt_pivot = df.pivot_table(index=\"user_session\", columns=\"event_type\",\n",
        "                           values=\"product_id\", aggfunc=\"count\", fill_value=0)\n",
        "evt_pivot[\"total\"] = evt_pivot.sum(axis=1)\n",
        "\n",
        "non_view_cols = [c for c in evt_pivot.columns if c not in [\"VIEW\",\"total\"]]\n",
        "only_view = ((evt_pivot.get(\"VIEW\",0) > 0) & (evt_pivot[non_view_cols].sum(axis=1)==0)).mean()\n",
        "has_buy   = (evt_pivot.get(\"BUY\",0) > 0).mean()\n",
        "print(\"Sadece VIEW içeren oturum oranı:\", round(only_view,4))\n",
        "print(\"BUY içeren oturum oranı:\", round(has_buy,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIe_R7q6xnO5",
        "outputId": "2a0c378b-14d9-41ad-98d1-d00f4d8dd81a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satır düzeyi sayımlar:\n",
            "event_type\n",
            "VIEW           58829\n",
            "ADD_CART       42304\n",
            "REMOVE_CART    25615\n",
            "BUY            14471\n",
            "Name: count, dtype: int64\n",
            "Sadece VIEW içeren oturum oranı: 0.4746\n",
            "BUY içeren oturum oranı: 0.124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_users = set(df[\"user_id\"].unique())\n",
        "test_users  = set(test[\"user_id\"].unique())\n",
        "print(\"train∩test user oranı:\", round(len(train_users & test_users)/len(test_users), 4))\n",
        "\n",
        "sess_per_user = df.groupby(\"user_id\")[\"user_session\"].nunique()\n",
        "print(sess_per_user.describe(percentiles=[.5,.9,.99]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t-VDXvJxtny",
        "outputId": "cd5a4760-e4c3-4788-82e8-c17c172e7af5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train∩test user oranı: 0.2038\n",
            "count    51821.000000\n",
            "mean         1.365412\n",
            "std          1.202873\n",
            "min          1.000000\n",
            "50%          1.000000\n",
            "90%          2.000000\n",
            "99%          6.000000\n",
            "max         47.000000\n",
            "Name: user_session, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sv_by_sess = df.groupby(\"user_session\")[\"session_value\"].first()\n",
        "print(sv_by_sess.describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99]))\n",
        "print(\"Skewness:\", round(sv_by_sess.skew(),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXtEZjgWxtqA",
        "outputId": "f67493c3-7c0a-4f23-8760-c3003e19de7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    70736.000000\n",
            "mean        42.198130\n",
            "std         47.552369\n",
            "min          5.380000\n",
            "1%           5.480000\n",
            "5%           7.490000\n",
            "10%          9.890000\n",
            "25%         18.530000\n",
            "50%         30.750000\n",
            "75%         46.620000\n",
            "90%         86.480000\n",
            "95%        121.330000\n",
            "99%        225.401000\n",
            "max       2328.660000\n",
            "Name: session_value, dtype: float64\n",
            "Skewness: 7.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_us_unique = pd.Index(test['user_session'].astype(str).unique())\n",
        "sub_us         = pd.Index(sub['user_session'].astype(str))\n",
        "print(\"Aynı set:\", set(test_us_unique)==set(sub_us))\n",
        "print(\"Aynı uzunluk:\", len(test_us_unique)==len(sub_us))\n",
        "print(\"Aynı sıra:\", test_us_unique.equals(sub_us))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0TF9aEDxtuX",
        "outputId": "e1792872-ab06-41de-947e-4c349e52c87a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aynı set: True\n",
            "Aynı uzunluk: True\n",
            "Aynı sıra: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & test'te aynı user_session olanlar (210 adet vardı)\n",
        "overlap = pd.Index(test[\"user_session\"].unique()).intersection(df[\"user_session\"].unique())\n",
        "print(\"Çakışan oturum adedi:\", len(overlap))\n",
        "\n",
        "# Bu oturumların train'deki gerçek hedefleri:\n",
        "leak_map = (df.loc[df[\"user_session\"].isin(overlap)]\n",
        "              .groupby(\"user_session\")[\"session_value\"].first())\n",
        "ARTI = os.path.join(OUT, \"artifacts\"); os.makedirs(ARTI, exist_ok=True)\n",
        "leak_path = os.path.join(ARTI, \"leak_map_session_value.csv\")\n",
        "leak_map.to_csv(leak_path)\n",
        "print(\"leak_map kaydedildi ->\", leak_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKpnwAiPxxBT",
        "outputId": "fe9ba1d0-aed1-4faa-a6ac-e8d51c726e0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Çakışan oturum adedi: 210\n",
            "leak_map kaydedildi -> /content/sample_data/datathon2025/outputs/artifacts/leak_map_session_value.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_session_features_fixed(events, prod_freq=None, cat_freq=None, is_train=True):\n",
        "    e = events.copy()\n",
        "    e[\"event_time\"] = pd.to_datetime(e[\"event_time\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "    # Tür dummies (VIEW / ADD_CART / REMOVE_CART / BUY)\n",
        "    dummies = pd.get_dummies(e[\"event_type\"])\n",
        "    e = pd.concat([e[[\"user_session\",\"user_id\",\"product_id\",\"category_id\",\"event_time\"]], dummies], axis=1)\n",
        "\n",
        "    # Sayım & süre\n",
        "    agg = e.groupby(\"user_session\").agg(\n",
        "        user_id=(\"user_id\",\"first\"),\n",
        "        n_events=(\"event_time\",\"size\"),\n",
        "        t_min=(\"event_time\",\"min\"),\n",
        "        t_max=(\"event_time\",\"max\"),\n",
        "        uniq_products=(\"product_id\",\"nunique\"),\n",
        "        uniq_categories=(\"category_id\",\"nunique\"),\n",
        "    )\n",
        "\n",
        "    # Event type counts\n",
        "    for col in [\"VIEW\",\"ADD_CART\",\"REMOVE_CART\",\"BUY\"]:\n",
        "        if col in e.columns:\n",
        "            agg[f\"n_{col}\"] = e.groupby(\"user_session\")[col].sum()\n",
        "        else:\n",
        "            agg[f\"n_{col}\"] = 0\n",
        "\n",
        "    # Son event tipi\n",
        "    last_evt = (events.sort_values(\"event_time\")\n",
        "                      .groupby(\"user_session\")[\"event_type\"].last())\n",
        "    last_dum = pd.get_dummies(last_evt, prefix=\"last\")\n",
        "    agg = pd.concat([agg, last_dum], axis=1)\n",
        "\n",
        "    # === MEVCUT FEATURES ===\n",
        "    agg[\"duration_sec\"] = (agg[\"t_max\"] - agg[\"t_min\"]).dt.total_seconds().fillna(0)\n",
        "    agg[\"share_add\"]    = agg[\"n_ADD_CART\"]   / agg[\"n_events\"]\n",
        "    agg[\"share_remove\"] = agg[\"n_REMOVE_CART\"]/ agg[\"n_events\"]\n",
        "    agg[\"share_buy\"]    = agg[\"n_BUY\"]        / agg[\"n_events\"]\n",
        "    agg[\"repeat_rate\"]  = (agg[\"n_events\"] - agg[\"uniq_products\"]) / agg[\"n_events\"]\n",
        "\n",
        "    # Saat / hafta içi modu\n",
        "    tmp = e.copy()\n",
        "    tmp[\"hour\"] = tmp[\"event_time\"].dt.hour\n",
        "    tmp[\"wday\"] = tmp[\"event_time\"].dt.weekday\n",
        "    agg[\"hour_mode\"] = tmp.groupby(\"user_session\")[\"hour\"].agg(lambda x: x.mode().iloc[0] if len(x)>0 else 0)\n",
        "    agg[\"wday_mode\"] = tmp.groupby(\"user_session\")[\"wday\"].agg(lambda x: x.mode().iloc[0] if len(x)>0 else 0)\n",
        "\n",
        "    # Popülerlik\n",
        "    if is_train or (prod_freq is None or cat_freq is None):\n",
        "        prod_freq = e[\"product_id\"].value_counts()\n",
        "        cat_freq  = e[\"category_id\"].value_counts()\n",
        "    e[\"prod_freq\"] = e[\"product_id\"].map(prod_freq).fillna(0).astype(float)\n",
        "    e[\"cat_freq\"]  = e[\"category_id\"].map(cat_freq).fillna(0).astype(float)\n",
        "    agg[\"avg_prod_freq\"] = e.groupby(\"user_session\")[\"prod_freq\"].mean()\n",
        "    agg[\"avg_cat_freq\"]  = e.groupby(\"user_session\")[\"cat_freq\"].mean()\n",
        "\n",
        "    # === FIXED: VALUE-BASED FEATURES KALDIRILDI ===\n",
        "    # Test'te session_value olmadığı için bu feature'lar train/test mismatch yaratıyor!\n",
        "    # Onun yerine sadece diğer güçlü feature'ları tutalım\n",
        "\n",
        "    # 2. TIME-BASED FEATURES (Güvenli!)\n",
        "    agg[\"is_morning\"] = ((agg[\"hour_mode\"] >= 6) & (agg[\"hour_mode\"] < 12)).astype(int)\n",
        "    agg[\"is_afternoon\"] = ((agg[\"hour_mode\"] >= 12) & (agg[\"hour_mode\"] < 18)).astype(int)\n",
        "    agg[\"is_evening\"] = ((agg[\"hour_mode\"] >= 18) & (agg[\"hour_mode\"] < 22)).astype(int)\n",
        "    agg[\"is_night\"] = ((agg[\"hour_mode\"] >= 22) | (agg[\"hour_mode\"] < 6)).astype(int)\n",
        "    agg[\"is_weekend\"] = (agg[\"wday_mode\"] >= 5).astype(int)\n",
        "\n",
        "    # Average event spacing\n",
        "    agg[\"avg_event_spacing\"] = agg[\"duration_sec\"] / (agg[\"n_events\"] + 0.1)\n",
        "\n",
        "    # 3. BEHAVIORAL FEATURES (Güvenli!)\n",
        "    agg[\"conversion_rate\"] = agg[\"n_BUY\"] / (agg[\"n_ADD_CART\"] + agg[\"n_BUY\"] + 0.1)\n",
        "    agg[\"exploration_rate\"] = agg[\"uniq_products\"] / agg[\"n_events\"]\n",
        "    agg[\"cart_efficiency\"] = (agg[\"n_ADD_CART\"] - agg[\"n_REMOVE_CART\"]) / (agg[\"n_ADD_CART\"] + 0.1)\n",
        "    agg[\"view_to_cart_rate\"] = agg[\"n_ADD_CART\"] / (agg[\"n_VIEW\"] + 0.1)\n",
        "\n",
        "    # Activity intensity\n",
        "    agg[\"activity_intensity\"] = agg[\"n_events\"] / (agg[\"duration_sec\"] + 1)  # events per second\n",
        "\n",
        "    # Purchase decision metrics\n",
        "    agg[\"purchase_decisiveness\"] = agg[\"n_BUY\"] / (agg[\"n_events\"] + 0.1)\n",
        "    agg[\"cart_to_buy_ratio\"] = agg[\"n_BUY\"] / (agg[\"n_ADD_CART\"] + 0.1)\n",
        "\n",
        "    # 4. CATEGORY/PRODUCT DIVERSITY\n",
        "    agg[\"product_diversity\"] = agg[\"uniq_products\"] / (agg[\"n_events\"] + 0.1)\n",
        "    agg[\"category_diversity\"] = agg[\"uniq_categories\"] / (agg[\"n_events\"] + 0.1)\n",
        "\n",
        "    # Product repeat behavior\n",
        "    agg[\"product_repeat_rate\"] = 1 - agg[\"product_diversity\"]\n",
        "\n",
        "    # 5. SEQUENCE FEATURES\n",
        "    # İlk event\n",
        "    first_evt = events.sort_values(\"event_time\").groupby(\"user_session\")[\"event_type\"].first()\n",
        "    first_dum = pd.get_dummies(first_evt, prefix=\"first\")\n",
        "    agg = pd.concat([agg, first_dum], axis=1)\n",
        "\n",
        "    # Journey completion metrics\n",
        "    agg[\"started_with_view\"] = agg.get(\"first_VIEW\", 0)\n",
        "    agg[\"ended_with_buy\"] = agg.get(\"last_BUY\", 0)\n",
        "    agg[\"complete_journey\"] = agg[\"started_with_view\"] * agg[\"ended_with_buy\"]\n",
        "\n",
        "    # Temizlik\n",
        "    agg = agg.drop(columns=[\"t_min\",\"t_max\"])\n",
        "    agg = agg.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "    return agg.reset_index(), prod_freq, cat_freq"
      ],
      "metadata": {
        "id": "DS6W4bfmxxDo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sonra bu kodu çalıştır:\n",
        "print(\"=== ENHANCED FEATURES ÜRETİLİYOR ===\")\n",
        "\n",
        "# Train ve test özellikleri yeni fonksiyon ile\n",
        "train_feat, prod_freq, cat_freq = build_session_features_fixed(df, is_train=True)\n",
        "test_feat, _, _ = build_session_features_fixed(test, prod_freq=prod_freq, cat_freq=cat_freq, is_train=False)\n",
        "\n",
        "print(f\"Train features shape: {train_feat.shape}\")\n",
        "print(f\"Test features shape: {test_feat.shape}\")\n",
        "print(f\"Feature sayısı: {train_feat.shape[1]} (eski: 22)\")\n",
        "print(\"Yeni feature'lar eklendi!\")\n",
        "print(\"\\nİlk 5 feature adı:\", train_feat.columns[:5].tolist())\n",
        "print(\"Son 5 feature adı:\", train_feat.columns[-5:].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_zO1lWzyRZl",
        "outputId": "066c9753-03d4-4d8c-e7f0-27a648131b0f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENHANCED FEATURES ÜRETİLİYOR ===\n",
            "Train features shape: (70736, 45)\n",
            "Test features shape: (30789, 45)\n",
            "Feature sayısı: 45 (eski: 22)\n",
            "Yeni feature'lar eklendi!\n",
            "\n",
            "İlk 5 feature adı: ['user_session', 'user_id', 'n_events', 'uniq_products', 'uniq_categories']\n",
            "Son 5 feature adı: ['first_REMOVE_CART', 'first_VIEW', 'started_with_view', 'ended_with_buy', 'complete_journey']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hedefi oturum seviyesine indir\n",
        "y = df.groupby(\"user_session\")[\"session_value\"].first().rename(\"session_value\").reset_index()\n",
        "train_sess = (train_feat.merge(y, on=\"user_session\", how=\"left\", validate=\"one_to_one\"))\n",
        "\n",
        "# Zaman tabanlı kesim: train oturumlarının son event zamanına göre\n",
        "last_time = df.groupby(\"user_session\")[\"event_time\"].max()\n",
        "cutoff = last_time.max() - pd.Timedelta(days=2)  # son 2 günü valid\n",
        "print(\"Cutoff:\", cutoff)\n",
        "\n",
        "train_feat_ = train_feat.set_index(\"user_session\").join(last_time.rename(\"tmax\")).reset_index()\n",
        "tr_idx = train_feat_[\"tmax\"] <= cutoff\n",
        "va_idx = train_feat_[\"tmax\"] >  cutoff\n",
        "# YENİ:\n",
        "drop_cols = [\"user_session\",\"tmax\",\"user_id\"]  # ham ID'yi çıkar\n",
        "X_tr = train_feat_.loc[tr_idx].drop(columns=drop_cols)\n",
        "X_va = train_feat_.loc[va_idx].drop(columns=drop_cols)\n",
        "\n",
        "# Güvenlik: sadece sayısal kolonları tut ve bool'ları uint8 yap\n",
        "def make_numeric(X):\n",
        "    if len(X.select_dtypes(include=[\"bool\"]).columns):\n",
        "        X = X.astype({c:\"uint8\" for c in X.select_dtypes(include=[\"bool\"]).columns})\n",
        "    num_cols = X.select_dtypes(include=[\"number\"]).columns\n",
        "    return X[num_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "X_tr = make_numeric(X_tr)\n",
        "X_va = make_numeric(X_va)\n",
        "\n",
        "y_all = train_sess.set_index(\"user_session\")[\"session_value\"]\n",
        "y_tr = y_all.loc[train_feat_.loc[tr_idx,\"user_session\"]]\n",
        "y_va = y_all.loc[train_feat_.loc[va_idx,\"user_session\"]]\n",
        "\n",
        "print(\"Train oturum:\", X_tr.shape, \" | Valid oturum:\", X_va.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2k6yl17xxGd",
        "outputId": "fc807dd6-1862-480a-a387-978fe0a660ba"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cutoff: 2025-06-19 23:59:52+00:00\n",
            "Train oturum: (64450, 43)  | Valid oturum: (6286, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hedef transform (log1p)\n",
        "ytr = np.log1p(y_tr.values)\n",
        "yva = np.log1p(y_va.values)\n"
      ],
      "metadata": {
        "id": "j253XG5eyGlT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  XGBoost (mevcut)\n",
        "xgb1 = XGBRegressor(\n",
        "    n_estimators=600, max_depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
        "    tree_method=\"hist\", random_state=42, verbosity=0\n",
        ")\n",
        "xgb1.fit(X_tr, ytr)\n",
        "p_xgb1 = xgb1.predict(X_va)\n",
        "rmse_xgb1 = np.sqrt(mean_squared_error(yva, p_xgb1))\n",
        "print(f\"XGBoost-1 RMSE: {rmse_xgb1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4pN8VmD25ms",
        "outputId": "666f3325-7c3c-4306-a228-1a89c4d51733"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost-1 RMSE: 0.4775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# XGBoost (farklı parametreler)\n",
        "xgb2 = XGBRegressor(\n",
        "    n_estimators=800, max_depth=5, learning_rate=0.03,\n",
        "    subsample=0.9, colsample_bytree=0.7, reg_lambda=2.0,\n",
        "    tree_method=\"hist\", random_state=123, verbosity=0\n",
        ")\n",
        "xgb2.fit(X_tr, ytr)\n",
        "p_xgb2 = xgb2.predict(X_va)\n",
        "rmse_xgb2 = np.sqrt(mean_squared_error(yva, p_xgb2))\n",
        "print(f\"XGBoost-2 RMSE: {rmse_xgb2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv1uE2nD3BHJ",
        "outputId": "4538bdf3-767b-4d97-fb96-17bbbeefcfd7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost-2 RMSE: 0.4746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM\n",
        "lgb = LGBMRegressor(\n",
        "    n_estimators=700, max_depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.5,\n",
        "    random_state=42, verbosity=-1\n",
        ")\n",
        "lgb.fit(X_tr, ytr)\n",
        "p_lgb = lgb.predict(X_va)\n",
        "rmse_lgb = np.sqrt(mean_squared_error(yva, p_lgb))\n",
        "print(f\"LightGBM RMSE: {rmse_lgb:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsuzAb4B3E2J",
        "outputId": "e3c9fe9b-6a34-452f-e274-0bf32f982658"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM RMSE: 0.4762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost\n",
        "cat = CatBoostRegressor(\n",
        "    iterations=600, depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, reg_lambda=1.0, random_state=42, verbose=False\n",
        ")\n",
        "cat.fit(X_tr, ytr)\n",
        "p_cat = cat.predict(X_va)\n",
        "rmse_cat = np.sqrt(mean_squared_error(yva, p_cat))\n",
        "print(f\"CatBoost RMSE: {rmse_cat:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Xhbhrm3MLy",
        "outputId": "a55a2d97-4e84-4751-dd84-b7a936d296be"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost RMSE: 0.4745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300, max_depth=12, min_samples_split=5,\n",
        "    random_state=42, n_jobs=-1\n",
        ")\n",
        "rf.fit(X_tr, ytr)\n",
        "p_rf = rf.predict(X_va)\n",
        "rmse_rf = np.sqrt(mean_squared_error(yva, p_rf))\n",
        "print(f\"Random Forest RMSE: {rmse_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xWaQ4v_3OqR",
        "outputId": "9e467b25-92aa-4714-a0c8-170f620db704"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest RMSE: 0.4784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra Trees\n",
        "et = ExtraTreesRegressor(\n",
        "    n_estimators=300, max_depth=12, min_samples_split=5,\n",
        "    random_state=42, n_jobs=-1\n",
        ")\n",
        "et.fit(X_tr, ytr)\n",
        "p_et = et.predict(X_va)\n",
        "rmse_et = np.sqrt(mean_squared_error(yva, p_et))\n",
        "print(f\"Extra Trees RMSE: {rmse_et:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIGqKI4n3Tro",
        "outputId": "ceaf73cc-b5f7-41fe-896f-8918d6df2bdb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extra Trees RMSE: 0.4795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENSEMBLE: Basit ortalama\n",
        "p_ensemble_simple = (p_xgb1 + p_xgb2 + p_lgb + p_cat + p_rf + p_et) / 6\n",
        "rmse_ensemble_simple = np.sqrt(mean_squared_error(yva, p_ensemble_simple))\n",
        "print(f\"Basit Ortalama RMSE: {rmse_ensemble_simple:.4f}\")\n",
        "\n",
        "#ENSEMBLE: Ağırlıklı ortalama (RMSE'ye göre)\n",
        "rmse_scores = np.array([rmse_xgb1, rmse_xgb2, rmse_lgb, rmse_cat, rmse_rf, rmse_et])\n",
        "weights = 1 / rmse_scores\n",
        "weights = weights / weights.sum()  # normalize et\n",
        "\n",
        "model_names = [\"XGB1\", \"XGB2\", \"LGB\", \"CAT\", \"RF\", \"ET\"]\n",
        "for name, w in zip(model_names, weights):\n",
        "    print(f\"{name}: {w:.3f}\")\n",
        "\n",
        "p_ensemble_weighted = (p_xgb1 * weights[0] + p_xgb2 * weights[1] +\n",
        "                      p_lgb * weights[2] + p_cat * weights[3] +\n",
        "                      p_rf * weights[4] + p_et * weights[5])\n",
        "rmse_ensemble_weighted = np.sqrt(mean_squared_error(yva, p_ensemble_weighted))\n",
        "print(f\"Ağırlıklı Ortalama RMSE: {rmse_ensemble_weighted:.4f}\")\n",
        "\n",
        "#ENSEMBLE: Stacking (Ridge ile)\n",
        "\n",
        "stack_X = np.column_stack([p_xgb1, p_xgb2, p_lgb, p_cat, p_rf, p_et])\n",
        "ridge = Ridge(alpha=1.0, random_state=42)\n",
        "ridge.fit(stack_X, yva)\n",
        "p_ensemble_stack = ridge.predict(stack_X)\n",
        "rmse_ensemble_stack = np.sqrt(mean_squared_error(yva, p_ensemble_stack))\n",
        "print(f\"Stacking (Ridge) RMSE: {rmse_ensemble_stack:.4f}\")\n",
        "\n",
        "print(f\"Stacking katsayıları: {ridge.coef_}\")\n",
        "\n",
        "# En iyi ensemble seç\n",
        "best_rmse = min(rmse_ensemble_simple, rmse_ensemble_weighted, rmse_ensemble_stack)\n",
        "if best_rmse == rmse_ensemble_simple:\n",
        "    best_method = \"simple\"\n",
        "    print(f\"\\nEn iyi method: Basit Ortalama (RMSE: {best_rmse:.4f})\")\n",
        "elif best_rmse == rmse_ensemble_weighted:\n",
        "    best_method = \"weighted\"\n",
        "    print(f\"\\nEn iyi method: Ağırlıklı Ortalama (RMSE: {best_rmse:.4f})\")\n",
        "else:\n",
        "    best_method = \"stacking\"\n",
        "    print(f\"\\nEn iyi method: Stacking (RMSE: {best_rmse:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxmpMlJ4DGG",
        "outputId": "daeba112-aef7-488a-e837-871bb3d6bfd3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basit Ortalama RMSE: 0.4749\n",
            "XGB1: 0.166\n",
            "XGB2: 0.167\n",
            "LGB: 0.167\n",
            "CAT: 0.167\n",
            "RF: 0.166\n",
            "ET: 0.166\n",
            "Ağırlıklı Ortalama RMSE: 0.4749\n",
            "Stacking (Ridge) RMSE: 0.4737\n",
            "Stacking katsayıları: [-0.24879839  0.69575129  0.02535835  0.49452387 -0.00885122  0.03803709]\n",
            "\n",
            "En iyi method: Stacking (RMSE: 0.4737)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENSEMBLE MODEL İYİLEŞTİRMESİ ===\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=== ENSEMBLE MODEL EĞİTİMİ BAŞLIYOR ===\")\n",
        "\n",
        "# Hedef transform (log1p)\n",
        "ytr = np.log1p(y_tr.values)\n",
        "yva = np.log1p(y_va.values)\n",
        "\n",
        "#  XGBoost\n",
        "xgb1 = XGBRegressor(\n",
        "    n_estimators=600, max_depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
        "    tree_method=\"hist\", random_state=42, verbosity=0\n",
        ")\n",
        "xgb1.fit(X_tr, ytr)\n",
        "p_xgb1 = xgb1.predict(X_va)\n",
        "rmse_xgb1 = np.sqrt(mean_squared_error(yva, p_xgb1))\n",
        "print(f\"XGBoost-1 RMSE: {rmse_xgb1:.4f}\")\n",
        "\n",
        "# XGBoost (farklı parametreler)\n",
        "xgb2 = XGBRegressor(\n",
        "    n_estimators=800, max_depth=5, learning_rate=0.03,\n",
        "    subsample=0.9, colsample_bytree=0.7, reg_lambda=2.0,\n",
        "    tree_method=\"hist\", random_state=123, verbosity=0\n",
        ")\n",
        "xgb2.fit(X_tr, ytr)\n",
        "p_xgb2 = xgb2.predict(X_va)\n",
        "rmse_xgb2 = np.sqrt(mean_squared_error(yva, p_xgb2))\n",
        "print(f\"XGBoost-2 RMSE: {rmse_xgb2:.4f}\")\n",
        "\n",
        "#LightGBM\n",
        "lgb = LGBMRegressor(\n",
        "    n_estimators=700, max_depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.5,\n",
        "    random_state=42, verbosity=-1\n",
        ")\n",
        "lgb.fit(X_tr, ytr)\n",
        "p_lgb = lgb.predict(X_va)\n",
        "rmse_lgb = np.sqrt(mean_squared_error(yva, p_lgb))\n",
        "print(f\"LightGBM RMSE: {rmse_lgb:.4f}\")\n",
        "\n",
        "#CatBoost\n",
        "cat = CatBoostRegressor(\n",
        "    iterations=600, depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, reg_lambda=1.0, random_state=42, verbose=False\n",
        ")\n",
        "cat.fit(X_tr, ytr)\n",
        "p_cat = cat.predict(X_va)\n",
        "rmse_cat = np.sqrt(mean_squared_error(yva, p_cat))\n",
        "print(f\"CatBoost RMSE: {rmse_cat:.4f}\")\n",
        "\n",
        "#Random Forest\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300, max_depth=12, min_samples_split=5,\n",
        "    random_state=42, n_jobs=-1\n",
        ")\n",
        "rf.fit(X_tr, ytr)\n",
        "p_rf = rf.predict(X_va)\n",
        "rmse_rf = np.sqrt(mean_squared_error(yva, p_rf))\n",
        "print(f\"Random Forest RMSE: {rmse_rf:.4f}\")\n",
        "\n",
        "#Extra Trees\n",
        "et = ExtraTreesRegressor(\n",
        "    n_estimators=300, max_depth=12, min_samples_split=5,\n",
        "    random_state=42, n_jobs=-1\n",
        ")\n",
        "et.fit(X_tr, ytr)\n",
        "p_et = et.predict(X_va)\n",
        "rmse_et = np.sqrt(mean_squared_error(yva, p_et))\n",
        "print(f\"Extra Trees RMSE: {rmse_et:.4f}\")\n",
        "\n",
        "#ENSEMBLE: Basit ortalama\n",
        "p_ensemble_simple = (p_xgb1 + p_xgb2 + p_lgb + p_cat + p_rf + p_et) / 6\n",
        "rmse_ensemble_simple = np.sqrt(mean_squared_error(yva, p_ensemble_simple))\n",
        "print(f\"Basit Ortalama RMSE: {rmse_ensemble_simple:.4f}\")\n",
        "\n",
        "# ENSEMBLE: Ağırlıklı ortalama (RMSE'ye göre)\n",
        "rmse_scores = np.array([rmse_xgb1, rmse_xgb2, rmse_lgb, rmse_cat, rmse_rf, rmse_et])\n",
        "weights = 1 / rmse_scores\n",
        "weights = weights / weights.sum()  # normalize et\n",
        "\n",
        "print(\"Model ağırlıkları:\")\n",
        "model_names = [\"XGB1\", \"XGB2\", \"LGB\", \"CAT\", \"RF\", \"ET\"]\n",
        "for name, w in zip(model_names, weights):\n",
        "    print(f\"{name}: {w:.3f}\")\n",
        "\n",
        "p_ensemble_weighted = (p_xgb1 * weights[0] + p_xgb2 * weights[1] +\n",
        "                      p_lgb * weights[2] + p_cat * weights[3] +\n",
        "                      p_rf * weights[4] + p_et * weights[5])\n",
        "rmse_ensemble_weighted = np.sqrt(mean_squared_error(yva, p_ensemble_weighted))\n",
        "print(f\"Ağırlıklı Ortalama RMSE: {rmse_ensemble_weighted:.4f}\")\n",
        "\n",
        "# ENSEMBLE: Stacking (Ridge ile)\n",
        "stack_X = np.column_stack([p_xgb1, p_xgb2, p_lgb, p_cat, p_rf, p_et])\n",
        "ridge = Ridge(alpha=1.0, random_state=42)\n",
        "ridge.fit(stack_X, yva)\n",
        "p_ensemble_stack = ridge.predict(stack_X)\n",
        "rmse_ensemble_stack = np.sqrt(mean_squared_error(yva, p_ensemble_stack))\n",
        "print(f\"Stacking (Ridge) RMSE: {rmse_ensemble_stack:.4f}\")\n",
        "\n",
        "print(f\"Stacking katsayıları: {ridge.coef_}\")\n",
        "\n",
        "# En iyi ensemble seç\n",
        "best_rmse = min(rmse_ensemble_simple, rmse_ensemble_weighted, rmse_ensemble_stack)\n",
        "if best_rmse == rmse_ensemble_simple:\n",
        "    best_method = \"simple\"\n",
        "    print(f\"\\nEn iyi method: Basit Ortalama (RMSE: {best_rmse:.4f})\")\n",
        "elif best_rmse == rmse_ensemble_weighted:\n",
        "    best_method = \"weighted\"\n",
        "    print(f\"\\nEn iyi method: Ağırlıklı Ortalama (RMSE: {best_rmse:.4f})\")\n",
        "else:\n",
        "    best_method = \"stacking\"\n",
        "    print(f\"\\nEn iyi method: Stacking (RMSE: {best_rmse:.4f})\")\n",
        "\n",
        "# FULL TRAIN: Tüm veri ile eğit\n",
        "print(\"\\n=== TÜM VERİ İLE EĞİTİM ===\")\n",
        "y_full = y_all.loc[train_feat_[\"user_session\"]].values\n",
        "y_full_log = np.log1p(y_full)\n",
        "\n",
        "# X_full ve X_te hazırla\n",
        "drop_cols = [\"user_session\", \"tmax\", \"user_id\"]\n",
        "X_full = train_feat_.drop(columns=drop_cols)\n",
        "X_te = test_feat.drop(columns=[\"user_session\", \"user_id\"], errors=\"ignore\")\n",
        "\n",
        "# Sayısal hale getir\n",
        "def make_numeric(X):\n",
        "    bool_cols = X.select_dtypes(include=[\"bool\"]).columns\n",
        "    if len(bool_cols):\n",
        "        X = X.astype({c: \"uint8\" for c in bool_cols})\n",
        "    num_cols = X.select_dtypes(include=[\"number\"]).columns\n",
        "    X = X[num_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "    return X\n",
        "\n",
        "X_full = make_numeric(X_full)\n",
        "X_te = make_numeric(X_te)\n",
        "X_te = X_te.reindex(columns=X_full.columns, fill_value=0)\n",
        "\n",
        "print(f\"X_full shape: {X_full.shape}, X_te shape: {X_te.shape}\")\n",
        "\n",
        "# Modelleri tekrar eğit\n",
        "xgb1_full = XGBRegressor(n_estimators=600, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, tree_method=\"hist\", random_state=42, verbosity=0)\n",
        "xgb2_full = XGBRegressor(n_estimators=800, max_depth=5, learning_rate=0.03, subsample=0.9, colsample_bytree=0.7, reg_lambda=2.0, tree_method=\"hist\", random_state=123, verbosity=0)\n",
        "lgb_full = LGBMRegressor(n_estimators=700, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.5, random_state=42, verbosity=-1)\n",
        "cat_full = CatBoostRegressor(iterations=600, depth=6, learning_rate=0.05, subsample=0.8, reg_lambda=1.0, random_state=42, verbose=False)\n",
        "rf_full = RandomForestRegressor(n_estimators=300, max_depth=12, min_samples_split=5, random_state=42, n_jobs=-1)\n",
        "et_full = ExtraTreesRegressor(n_estimators=300, max_depth=12, min_samples_split=5, random_state=42, n_jobs=-1)\n",
        "\n",
        "models_full = [xgb1_full, xgb2_full, lgb_full, cat_full, rf_full, et_full]\n",
        "for i, model in enumerate(models_full):\n",
        "    print(f\"Model {i+1} eğitiliyor...\")\n",
        "    model.fit(X_full, y_full_log)\n",
        "\n",
        "# Test tahminleri\n",
        "print(\"Test tahminleri yapılıyor...\")\n",
        "test_preds = []\n",
        "for model in models_full:\n",
        "    pred = model.predict(X_te)\n",
        "    test_preds.append(pred)\n",
        "\n",
        "# Ensemble test tahmini\n",
        "if best_method == \"simple\":\n",
        "    p_te_ensemble = sum(test_preds) / len(test_preds)\n",
        "elif best_method == \"weighted\":\n",
        "    p_te_ensemble = sum(pred * w for pred, w in zip(test_preds, weights))\n",
        "else:  # stacking\n",
        "    stack_X_test = np.column_stack(test_preds)\n",
        "    p_te_ensemble = ridge.predict(stack_X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGrZDU3C4Yr9",
        "outputId": "588b7742-106c-44a5-9725-82c0b963266c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENSEMBLE MODEL EĞİTİMİ BAŞLIYOR ===\n",
            "XGBoost-1 RMSE: 0.4775\n",
            "XGBoost-2 RMSE: 0.4746\n",
            "LightGBM RMSE: 0.4762\n",
            "CatBoost RMSE: 0.4745\n",
            "Random Forest RMSE: 0.4784\n",
            "Extra Trees RMSE: 0.4795\n",
            "Basit Ortalama RMSE: 0.4749\n",
            "Model ağırlıkları:\n",
            "XGB1: 0.166\n",
            "XGB2: 0.167\n",
            "LGB: 0.167\n",
            "CAT: 0.167\n",
            "RF: 0.166\n",
            "ET: 0.166\n",
            "Ağırlıklı Ortalama RMSE: 0.4749\n",
            "Stacking (Ridge) RMSE: 0.4737\n",
            "Stacking katsayıları: [-0.24879839  0.69575129  0.02535835  0.49452387 -0.00885122  0.03803709]\n",
            "\n",
            "En iyi method: Stacking (RMSE: 0.4737)\n",
            "\n",
            "=== TÜM VERİ İLE EĞİTİM ===\n",
            "X_full shape: (70736, 43), X_te shape: (30789, 43)\n",
            "Model 1 eğitiliyor...\n",
            "Model 2 eğitiliyor...\n",
            "Model 3 eğitiliyor...\n",
            "Model 4 eğitiliyor...\n",
            "Model 5 eğitiliyor...\n",
            "Model 6 eğitiliyor...\n",
            "Test tahminleri yapılıyor...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Log'dan geri dönüş\n",
        "p_te_final = np.expm1(p_te_ensemble)"
      ],
      "metadata": {
        "id": "l2okif3j4keN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SUBMISSION HAZIRLA ===\n",
        "print(\"\\n=== SUBMISSION HAZIRLIĞI ===\")\n",
        "preds_ensemble = pd.DataFrame({\n",
        "    \"user_session\": test_feat[\"user_session\"].values,\n",
        "    \"session_value\": p_te_final\n",
        "})\n",
        "\n",
        "# Sub sırasına hizala\n",
        "submission = sub[[\"user_session\"]].merge(\n",
        "    preds_ensemble, on=\"user_session\", how=\"left\", validate=\"one_to_one\"\n",
        ")\n",
        "submission[\"session_value\"] = pd.to_numeric(submission[\"session_value\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "# 210 overlap doldur\n",
        "leak_csv = os.path.join(ARTI, \"leak_map_session_value.csv\")\n",
        "if os.path.exists(leak_csv):\n",
        "    leak_df = pd.read_csv(leak_csv)\n",
        "    if {\"user_session\", \"session_value\"}.issubset(leak_df.columns):\n",
        "        leak_map = pd.Series(leak_df[\"session_value\"].values, index=leak_df[\"user_session\"].astype(str).values)\n",
        "    else:\n",
        "        leak_df = pd.read_csv(leak_csv, index_col=0)\n",
        "        leak_map = leak_df.iloc[:, 0]\n",
        "        leak_map.index = leak_map.index.astype(str)\n",
        "\n",
        "    leak_map = pd.to_numeric(leak_map, errors=\"coerce\").astype(\"float64\")\n",
        "    hits = submission[\"user_session\"].astype(str).isin(leak_map.index)\n",
        "    mapped = submission.loc[hits, \"user_session\"].astype(str).map(leak_map).to_numpy(dtype=\"float64\")\n",
        "    submission.loc[hits, \"session_value\"] = mapped\n",
        "    print(\"Overlap fill uygulanan satır:\", int(hits.sum()))\n",
        "\n",
        "# Temizle ve kaydet\n",
        "submission[\"session_value\"] = submission[\"session_value\"].fillna(0.0)\n",
        "submission[\"session_value\"] = np.clip(submission[\"session_value\"].values, 0.0, None).astype(\"float64\")\n",
        "submission[\"session_value\"] = submission[\"session_value\"].round(5)\n",
        "\n",
        "out_csv = os.path.join(OUT, \"submission_ensemble2.csv\")\n",
        "submission.to_csv(out_csv, index=False)\n",
        "print(f\"Ensemble submission yazıldı -> {out_csv} | satır: {submission.shape[0]}\")\n",
        "print(f\"Kullanılan method: {best_method}\")\n",
        "print(f\"Validation RMSE: {best_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBi15xDo47jQ",
        "outputId": "f4861fff-46c6-40ff-b4a5-99b7e8ab5596"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SUBMISSION HAZIRLIĞI ===\n",
            "Overlap fill uygulanan satır: 210\n",
            "Ensemble submission yazıldı -> /content/sample_data/datathon2025/outputs/submission_ensemble2.csv | satır: 30789\n",
            "Kullanılan method: stacking\n",
            "Validation RMSE: 0.4737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb = XGBRegressor(tree_method=\"hist\", random_state=123, verbosity=0)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [800, 1000, 1200],\n",
        "    'max_depth': [4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.03, 0.05],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'reg_lambda': [1.0, 2.0, 3.0],\n",
        "    'reg_alpha': [0.0, 0.1, 0.5]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    xgb, param_distributions=param_dist,\n",
        "    n_iter=20, scoring='neg_root_mean_squared_error', cv=3, verbose=1, n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X_tr, ytr)\n",
        "print(search.best_params_)\n",
        "print(-search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vVUOxuSBK86",
        "outputId": "3da43d2d-78aa-4642-fe3b-76b138c5e9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uw2IVq8NBd7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
